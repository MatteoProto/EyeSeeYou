/*
 * Copyright 2020 Google LLC
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.example.eyeSeeYou.samplerender.arcore

import android.media.Image
import android.opengl.GLES30
import com.example.eyeSeeYou.samplerender.Framebuffer
import com.example.eyeSeeYou.samplerender.Mesh
import com.example.eyeSeeYou.samplerender.SampleRender
import com.example.eyeSeeYou.samplerender.Shader
import com.example.eyeSeeYou.samplerender.Texture
import com.example.eyeSeeYou.samplerender.VertexBuffer
import com.google.ar.core.Coordinates2d
import com.google.ar.core.Frame
import java.io.IOException
import java.nio.ByteBuffer
import java.nio.ByteOrder
import java.nio.FloatBuffer

/**
 * This class both renders the AR camera background and composes the a scene foreground. The camera
 * background can be rendered as either camera image data or camera depth data. The virtual scene
 * can be composited with or without depth occlusion.
 */
class BackgroundRenderer(render: SampleRender?) {
    private val cameraTexCoords: FloatBuffer = ByteBuffer.allocateDirect(
        COORDS_BUFFER_SIZE
    ).order(ByteOrder.nativeOrder()).asFloatBuffer()

    private val mesh: Mesh
    private val cameraTexCoordsVertexBuffer: VertexBuffer
    private var backgroundShader: Shader? = null
    private var occlusionShader: Shader? = null

    /** Return the camera depth texture generated by this object.  */
    private val cameraDepthTexture: Texture = Texture(
        render,
        Texture.Target.TEXTURE_2D,
        Texture.WrapMode.CLAMP_TO_EDGE,
        false
    )

    /** Return the camera color texture generated by this object.  */
    val cameraColorTexture: Texture =
        Texture(
            render,
            Texture.Target.TEXTURE_EXTERNAL_OES,
            Texture.WrapMode.CLAMP_TO_EDGE,
            false
        )

    private var useDepthVisualization = false
    private var useOcclusion = false
    private var aspectRatio = 0f

    /**
     * Allocates and initializes OpenGL resources needed by the background renderer. Must be called
     * during a {/@link SampleRender.Renderer} callback, typically in {/@link
     * SampleRender.Renderer#onSurfaceCreated()}.
     */
    init {
        // Create a Mesh with three vertex buffers: one for the screen coordinates (normalized device
        // coordinates), one for the camera texture coordinates (to be populated with proper data later
        // before drawing), and one for the virtual scene texture coordinates (unit texture quad)
        val screenCoordsVertexBuffer =
            VertexBuffer(render,  /* numberOfEntriesPerVertex=*/2, NDC_QUAD_COORDS_BUFFER)
        cameraTexCoordsVertexBuffer =
            VertexBuffer(render,  /*numberOfEntriesPerVertex=*/2,  /*entries=*/null)
        val virtualSceneTexCoordsVertexBuffer =
            VertexBuffer(render,  /* numberOfEntriesPerVertex=*/2, VIRTUAL_SCENE_TEX_COORDS_BUFFER)
        val vertexBuffers = arrayOf(
            screenCoordsVertexBuffer,
            cameraTexCoordsVertexBuffer,
            virtualSceneTexCoordsVertexBuffer,
        )
        mesh =
            Mesh(render, Mesh.PrimitiveMode.TRIANGLE_STRIP,  /*indexBuffer=*/null, vertexBuffers)
    }

    /**
     * Sets whether the background camera image should be replaced with a depth visualization instead.
     * This reloads the corresponding shader code, and must be called on the GL thread.
     */
    @Throws(IOException::class)
    fun setUseDepthVisualization(render: SampleRender, useDepthVisualization: Boolean) {
        if (backgroundShader != null) {
            if (this.useDepthVisualization == useDepthVisualization) {
                return
            }
            backgroundShader!!.close()
            backgroundShader = null
            this.useDepthVisualization = useDepthVisualization
        }
        if (useDepthVisualization) {
            val depthColorPaletteTexture = Texture.createFromAsset(
                render,
                "models/depth_color_palette.png",
                Texture.WrapMode.CLAMP_TO_EDGE,
                Texture.ColorFormat.LINEAR
            )
            backgroundShader =
                Shader.createFromAssets(
                    render,
                    "shaders/background_show_depth_color_visualization.vert",
                    "shaders/background_show_depth_color_visualization.frag",  /*defines=*/
                    null
                )
                    .setTexture("u_CameraDepthTexture", cameraDepthTexture)
                    .setTexture("u_ColorMap", depthColorPaletteTexture)
                    .setDepthTest(false)
                    .setDepthWrite(false)
        } else {
            val semanticColorPaletteTexture = Texture.createFromAsset(
                render,
                "models/semantic_color_palette.png",
                Texture.WrapMode.CLAMP_TO_EDGE,
                Texture.ColorFormat.LINEAR
            )
            backgroundShader =
                Shader.createFromAssets(
                    render,
                    "shaders/background_show_depth_color_visualization.vert",
                    "shaders/background_show_depth_color_visualization.frag",  /*defines=*/
                    null
                )
                    .setTexture("u_CameraDepthTexture", cameraDepthTexture)
                    .setTexture("u_ColorMap", semanticColorPaletteTexture)
                    .setDepthTest(false)
                    .setDepthWrite(false)
            //      backgroundShader =
//          Shader.createFromAssets(
//                  render,
//                  "shaders/background_show_camera.vert",
//                  "shaders/background_show_camera.frag",
//                  /*defines=*/ null)
//              .setTexture("u_CameraColorTexture", cameraColorTexture)
//              .setDepthTest(false)
//              .setDepthWrite(false);
        }
    }

    /**
     * Sets whether to use depth for occlusion. This reloads the shader code with new `#define`s, and must be called on the GL thread.
     */
    @Throws(IOException::class)
    fun setUseOcclusion(render: SampleRender, useOcclusion: Boolean) {
        if (occlusionShader != null) {
            if (this.useOcclusion == useOcclusion) {
                return
            }
            occlusionShader!!.close()
            occlusionShader = null
            this.useOcclusion = useOcclusion
        }
        val defines = HashMap<String, String>()
        defines["USE_OCCLUSION"] = if (useOcclusion) "1" else "0"
        occlusionShader =
            Shader.createFromAssets(
                render,
                "shaders/occlusion.vert",
                "shaders/occlusion.frag",
                defines
            )
                .setDepthTest(false)
                .setDepthWrite(false)
                .setBlend(Shader.BlendFactor.SRC_ALPHA, Shader.BlendFactor.ONE_MINUS_SRC_ALPHA)
        if (useOcclusion) {
            occlusionShader!!
                .setTexture("u_CameraDepthTexture", cameraDepthTexture)
                .setFloat("u_DepthAspectRatio", aspectRatio)
        }
    }

    /**
     * Updates the display geometry. This must be called every frame before calling either of
     * BackgroundRenderer's draw methods.
     *
     * @param frame The current `Frame` as returned by {/@link Session#update()}.
     */
    fun updateDisplayGeometry(frame: Frame) {
        if (frame.hasDisplayGeometryChanged()) {
            // If display rotation changed (also includes view size change), we need to re-query the UV
            // coordinates for the screen rect, as they may have changed as well.
            frame.transformCoordinates2d(
                Coordinates2d.OPENGL_NORMALIZED_DEVICE_COORDINATES,
                NDC_QUAD_COORDS_BUFFER,
                Coordinates2d.TEXTURE_NORMALIZED,
                cameraTexCoords
            )
            cameraTexCoordsVertexBuffer.set(cameraTexCoords)
        }
    }

    /** Update depth texture with Image contents.  */
    fun updateCameraDepthTexture(image: Image) {
        // SampleRender abstraction leaks here
        GLES30.glBindTexture(GLES30.GL_TEXTURE_2D, cameraDepthTexture.getTextureId())
        GLES30.glTexImage2D(
            GLES30.GL_TEXTURE_2D,
            0,
            GLES30.GL_RG8,
            image.width,
            image.height,
            0,
            GLES30.GL_RG,
            GLES30.GL_UNSIGNED_BYTE,
            image.planes[0].buffer
        )
        if (useOcclusion) {
            aspectRatio = image.width.toFloat() / image.height.toFloat()
            occlusionShader!!.setFloat("u_DepthAspectRatio", aspectRatio)
        }
    }

    fun updateCameraSemanticTexture(image: Image) {
        GLES30.glBindTexture(GLES30.GL_TEXTURE_2D, cameraDepthTexture.getTextureId())
        GLES30.glTexImage2D(
            GLES30.GL_TEXTURE_2D,
            0,
            GLES30.GL_LUMINANCE,  // FORMATO Y8 = LUMINANCE
            image.width,
            image.height,
            0,
            GLES30.GL_LUMINANCE,
            GLES30.GL_UNSIGNED_BYTE,
            image.planes[0].buffer
        )
    }


    /**
     * Draws the AR background image. The image will be drawn such that virtual content rendered with
     * the matrices provided by [com.google.ar.core.Camera.getViewMatrix] and
     * [com.google.ar.core.Camera.getProjectionMatrix] will
     * accurately follow static physical objects.
     */
    fun drawBackground(render: SampleRender) {
        backgroundShader?.let { render.draw(mesh, it) }
    }

    /**
     * Draws the virtual scene. Any objects rendered in the given [Framebuffer] will be drawn
     * given the previously specified {/@link OcclusionMode}.
     *
     *
     * Virtual content should be rendered using the matrices provided by [ ][com.google.ar.core.Camera.getViewMatrix] and [ ][com.google.ar.core.Camera.getProjectionMatrix].
     */
    fun drawVirtualScene(
        render: SampleRender, virtualSceneFramebuffer: Framebuffer, zNear: Float, zFar: Float
    ) {
        virtualSceneFramebuffer.colorTexture?.let {
            occlusionShader!!.setTexture(
                "u_VirtualSceneColorTexture", it
            )
        }
        if (useOcclusion) {
            virtualSceneFramebuffer.depthTexture?.let {
                occlusionShader!!
                    .setTexture("u_VirtualSceneDepthTexture", it)
                    .setFloat("u_ZNear", zNear)
                    .setFloat("u_ZFar", zFar)
            }
        }
        occlusionShader?.let { render.draw(mesh, it) }
    }

    companion object {
        // components_per_vertex * number_of_vertices * float_size
        private const val COORDS_BUFFER_SIZE = 2 * 4 * 4

        private val NDC_QUAD_COORDS_BUFFER: FloatBuffer = ByteBuffer.allocateDirect(
            COORDS_BUFFER_SIZE
        ).order(ByteOrder.nativeOrder()).asFloatBuffer()

        private val VIRTUAL_SCENE_TEX_COORDS_BUFFER: FloatBuffer = ByteBuffer.allocateDirect(
            COORDS_BUFFER_SIZE
        ).order(ByteOrder.nativeOrder()).asFloatBuffer()

        init {
            NDC_QUAD_COORDS_BUFFER.put(
                floatArrayOf(
                    /*0:*/-1f, -1f,  /*1:*/+1f, -1f,  /*2:*/-1f, +1f,  /*3:*/+1f, +1f,
                )
            )
            VIRTUAL_SCENE_TEX_COORDS_BUFFER.put(
                floatArrayOf(
                    /*0:*/0f, 0f,  /*1:*/1f, 0f,  /*2:*/0f, 1f,  /*3:*/1f, 1f,
                )
            )
        }
    }
}
